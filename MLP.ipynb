{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 5\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski 1.[5 pkt]\n",
        "4. Jakość kodu 0.[5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert (\n",
        "            learning_rate < 1\n",
        "        ), f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert (\n",
        "            learning_rate > 0\n",
        "        ), f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size: int, output_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # self.weights = np.random.randn(input_size, output_size)\n",
        "        self.weights = np.empty((input_size + 1, output_size))\n",
        "        self.weights.fill(1)\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.x = x\n",
        "        self.x = np.append(self.x, np.ones((self.x.shape[0], 1)), axis=1)\n",
        "        self.y = self.x * self.weights.transpose()\n",
        "\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        dx = output_error_derivative * self.weights.transpose()\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.x = x\n",
        "        self.y = 1 / (1 + np.exp(-(self.x)))\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        dx = output_error_derivative * (1 - self.x) * self.x\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    def __init__(\n",
        "        self, loss_function: callable, loss_function_derivative: callable\n",
        "    ) -> None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        return self.loss_function(x)\n",
        "\n",
        "    def loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        return self.loss_function_derivative(x, y).transpose()\n",
        "\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers: List[Layer], learning_rate: float) -> None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss: Loss) -> None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        pass\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        x_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        epochs: int,\n",
        "        learning_rate: float,\n",
        "        verbose: int = 0,\n",
        "    ) -> None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2 4 6 8]\n",
            " [2 4 6 8]\n",
            " [2 4 6 8]\n",
            " [2 4 6 8]\n",
            " [2 4 6 8]]\n",
            "(5, 6)\n",
            "(6, 5)\n",
            "(1, 5)\n",
            "[[0.73105858 0.88079708 0.95257413 0.98201379 0.73105858]\n",
            " [0.73105858 0.88079708 0.95257413 0.98201379 0.73105858]\n",
            " [0.73105858 0.88079708 0.95257413 0.98201379 0.73105858]\n",
            " [0.73105858 0.88079708 0.95257413 0.98201379 0.73105858]\n",
            " [0.73105858 0.88079708 0.95257413 0.98201379 0.73105858]\n",
            " [0.73105858 0.88079708 0.95257413 0.98201379 0.73105858]]\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "[[  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]]\n",
            "[[  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]\n",
            " [  0.  -2.  -6. -12.   0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "ar1 = np.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n",
        "ar2 = np.array([[2, 2, 2, 2, 2]])\n",
        "ar2 = ar2.transpose()\n",
        "print(ar1 * ar2)\n",
        "\n",
        "\n",
        "nx = 4\n",
        "n = 6\n",
        "hidden_layer = FullyConnected(nx, n)\n",
        "sigmoid_layer = Sigmoid()\n",
        "print(hidden_layer.weights.shape)\n",
        "print(hidden_layer.weights.transpose().shape)\n",
        "\n",
        "\n",
        "x = np.array([[1, 2, 3, 4]])\n",
        "y = hidden_layer.forward(x)\n",
        "y = sigmoid_layer.forward(y)\n",
        "print(hidden_layer.x.shape)\n",
        "print(y)\n",
        "\n",
        "\n",
        "ones = np.ones((n, 1))\n",
        "print(ones)\n",
        "dx = sigmoid_layer.backward(ones)\n",
        "print(dx)\n",
        "dx = hidden_layer.backward(dx)\n",
        "print(dx)\n",
        "\n",
        "MSE = lambda y, y_mod : sum((y - y_mod)**2)\n",
        "dMSE = lambda y, y_mod : 2 * (y - y_mod)\n",
        "loss = Loss(lambda x: x ** 2, lambda x, y: 2 * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
