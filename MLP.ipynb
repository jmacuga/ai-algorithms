{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Zadanie 5\n",
        "\n",
        "\n",
        "Celem ćwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
        "\n",
        "Następnie należy wytrenować perceptron wielowarstwowy do klasyfikacji zbioru danych [MNIST](http://yann.lecun.com/exdb/mnist/). Zbiór MNIST dostępny jest w pakiecie `scikit-learn`.\n",
        "\n",
        "Punktacja:\n",
        "1. Implementacja propagacji do przodu (`forward`) [1 pkt]\n",
        "2. Implementacja wstecznej propagacji (zademonstrowana na bramce XOR) (`backward`) [2 pkt]\n",
        "3. Przeprowadzenie eksperymentów na zbiorze MNIST, w tym:\n",
        "    1. Porównanie co najmniej dwóch architektur sieci [1 pkt]\n",
        "    2. Przetestowanie każdej architektury na conajmniej 3 ziarnach [1 pkt]\n",
        "    3. Wnioski 1.[5 pkt]\n",
        "4. Jakość kodu 0.[5 pkt]\n",
        "\n",
        "Polecane źródła - teoria + intuicja:\n",
        "1. [Karpathy, CS231n Winter 2016: Lecture 4: Backpropagation, Neural Networks 1](https://www.youtube.com/watch?v=i94OvYb6noo&ab_channel=AndrejKarpathy)\n",
        "2. [3 Blude one Brown, Backpropagation calculus | Chapter 4, Deep learning\n",
        "](https://www.youtube.com/watch?v=tIeHLnjs5U8&t=4s&ab_channel=3Blue1Brown)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod, ABC\n",
        "from typing import List\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets, metrics, svm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "    \"\"\"Basic building block of the Neural Network\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self._learning_rate = 0.01\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        \"\"\"Backward propagation of output_error_derivative through layer\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def learning_rate(self):\n",
        "        return self._learning_rate\n",
        "\n",
        "    @learning_rate.setter\n",
        "    def learning_rate(self, learning_rate):\n",
        "        assert (\n",
        "            learning_rate < 1\n",
        "        ), f\"Given learning_rate={learning_rate} is larger than 1\"\n",
        "        assert (\n",
        "            learning_rate > 0\n",
        "        ), f\"Given learning_rate={learning_rate} is smaller than 0\"\n",
        "        self._learning_rate = learning_rate\n",
        "\n",
        "\n",
        "class FullyConnected(Layer):\n",
        "    def __init__(self, input_size: int, output_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.weights = np.random.randn(input_size, output_size)\n",
        "        self.bias = np.random.randn(output_size)\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.x = x\n",
        "        self.y = self.x @ self.weights + self.bias\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        print(\"FullyConnected layer backward\")\n",
        "        print(f\"output_error_derivative shape: {output_error_derivative.shape}\")\n",
        "        print(f\"self.weights shape: {self.weights.shape}\")\n",
        "        print(f\"self.x shape: {self.x.shape}\")\n",
        "        print(f\"self.y shape: {self.y.shape}\")\n",
        "        dx = output_error_derivative.T @ self.weights.T\n",
        "        print(f\"dx shape: {dx.shape}\")\n",
        "        return dx.T\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
        "        self.x = x\n",
        "        self.y = 1 / (1 + np.exp(-(self.x)))\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, output_error_derivative) -> np.ndarray:\n",
        "        print(\"Sigmoid layer backward\")\n",
        "        print(f\"output_error_derivative shape: {output_error_derivative.shape}\")\n",
        "        print(f\"self.x shape: {self.x.shape}\")\n",
        "        print(f\"self.y shape: {self.y.shape}\")\n",
        "        local_grad = np.multiply((1 - self.x), self.x)\n",
        "        print(f\"local_grad shape: {local_grad.shape}\")\n",
        "        dx = output_error_derivative.T * local_grad\n",
        "        print(f\"dx shape: {dx.shape}\")\n",
        "        return dx.T\n",
        "\n",
        "\n",
        "class Loss:\n",
        "    def __init__(\n",
        "        self, loss_function: callable, loss_function_derivative: callable\n",
        "    ) -> None:\n",
        "        self.loss_function = loss_function\n",
        "        self.loss_function_derivative = loss_function_derivative\n",
        "\n",
        "    def loss(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function for a particular x\"\"\"\n",
        "        return self.loss_function(x)\n",
        "\n",
        "    def loss_derivative(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Loss function derivative for a particular x and y\"\"\"\n",
        "        return self.loss_function_derivative(x, y)\n",
        "\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers: List[Layer], learning_rate: float) -> None:\n",
        "        self.layers = layers\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def compile(self, loss: Loss) -> None:\n",
        "        \"\"\"Define the loss function and loss function derivative\"\"\"\n",
        "        self.loss = loss\n",
        "\n",
        "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Forward propagation of x through all layers\"\"\"\n",
        "        i = 0\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "            print(f\"Layer {i} output shape: {x.shape}\")\n",
        "            i += 1\n",
        "        return x\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        x_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        epochs: int,\n",
        "        learning_rate: float,\n",
        "        verbose: int = 0,\n",
        "    ) -> None:\n",
        "        \"\"\"Fit the network to the training data\"\"\"\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch}\")\n",
        "            for data in zip(x_train, y_train):\n",
        "                x, y = data\n",
        "                y = self.vectorized_result(y)\n",
        "                print(f\"y: {y}\")\n",
        "                x = np.array([x])\n",
        "                output = self(x)\n",
        "                current_error = output.T - y\n",
        "                print(f\"Current error: {current_error}\")\n",
        "                loss_value = self.loss.loss(current_error)\n",
        "                if loss_value < 0.01:\n",
        "                    print(\"Break\")\n",
        "                    print(f\"Loss: {loss_value}\")\n",
        "                    break\n",
        "                print(f\"Loss: {loss_value}\")\n",
        "                layers_reversed = self.layers[::-1]\n",
        "                loss_derivative_value = self.loss.loss_derivative(current_error, 0)\n",
        "                for layer in layers_reversed:\n",
        "                    loss_derivative_value = layer.backward(loss_derivative_value)\n",
        "                    if isinstance(layer, FullyConnected):\n",
        "                        # print(f\"Loss derivative value shape: {loss_derivative_value.shape}\")\n",
        "                        # print(f\"weights shape: {layer.weights.shape}\")\n",
        "                        # print(f\"learning_rate: {learning_rate}\")\n",
        "                        layer.weights -= learning_rate * loss_derivative_value\n",
        "\n",
        "    def vectorized_result(self, j):\n",
        "        \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
        "        and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
        "        into a corresponding desired output from the neural network.\n",
        "\n",
        "        \"\"\"\n",
        "        e = np.zeros((10, 1))\n",
        "        e[j] = 1.0\n",
        "        return e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Eksperymenty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# Wnioski"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 5)\n",
            "(5, 3)\n",
            "(1, 3)\n",
            "(1, 65)\n",
            "(65, 6)\n",
            "(1, 6)\n",
            "(4, 4)\n",
            "(4, 4)\n",
            "FullyConnected layer backward\n",
            "output_error_derivative shape: (1, 1)\n",
            "self.weights shape: (4, 1)\n",
            "self.x shape: (1, 4)\n",
            "self.y shape: (1, 1)\n",
            "dx shape: (1, 4)\n",
            "[[ 0.87044479  1.3228923  -2.11688101  1.89808048]]\n",
            "Sigmoid layer backward\n",
            "output_error_derivative shape: (1, 4)\n",
            "self.x shape: (1, 4)\n",
            "self.y shape: (1, 4)\n",
            "local_grad shape: (1, 4)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[129], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m dx \u001b[38;5;241m=\u001b[39m out_layer\u001b[38;5;241m.\u001b[39mbackward(np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m]]))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(dx)\n\u001b[0;32m---> 42\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid_layer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m dx \u001b[38;5;241m=\u001b[39m hidden_layer\u001b[38;5;241m.\u001b[39mbackward(dx)\n\u001b[1;32m     44\u001b[0m dx \u001b[38;5;241m=\u001b[39m sigmoid_layer\u001b[38;5;241m.\u001b[39mbackward(dx)\n",
            "Cell \u001b[0;32mIn[128], line 73\u001b[0m, in \u001b[0;36mSigmoid.backward\u001b[0;34m(self, output_error_derivative)\u001b[0m\n\u001b[1;32m     71\u001b[0m local_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_grad shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_grad\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[43moutput_error_derivative\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlocal_grad\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdx shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dx\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 4)"
          ]
        }
      ],
      "source": [
        "ar1 = np.array([[1, 2, 3, 4, 1]])\n",
        "print(ar1.shape)\n",
        "ar2 = np.array([[2, 2, 2, 2, 2], [3, 3, 3, 3, 3], [4, 4, 4, 4, 4]])\n",
        "ar2 = ar2.transpose()\n",
        "print(ar2.shape)\n",
        "ar3 = ar1 @ ar2\n",
        "print(ar3.shape)\n",
        "\n",
        "ar1 = np.random.randn(1, 65)\n",
        "ar2 = np.random.randn(65, 6)\n",
        "print(ar1.shape)\n",
        "print(ar2.shape)\n",
        "\n",
        "ar3 = ar1 @ ar2\n",
        "print(ar3.shape)\n",
        "\n",
        "\n",
        "nx = 10\n",
        "n = 4\n",
        "input_layer = FullyConnected(nx, n)\n",
        "sigmoid_layer = Sigmoid()\n",
        "hidden_layer = FullyConnected(n, n)\n",
        "sigmoid_layer2 = Sigmoid()\n",
        "out_layer = FullyConnected(n, 1)\n",
        "\n",
        "print(hidden_layer.weights.shape)\n",
        "print(hidden_layer.weights.transpose().shape)\n",
        "\n",
        "\n",
        "x = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
        "y = input_layer.forward(x)\n",
        "y = sigmoid_layer.forward(y)\n",
        "y = hidden_layer.forward(y)\n",
        "y = sigmoid_layer2.forward(y)\n",
        "y = out_layer.forward(y)\n",
        "# print(hidden_layer.x.shape)\n",
        "# print(y)\n",
        "\n",
        "\n",
        "dx = out_layer.backward(np.array([[1]]))\n",
        "print(dx)\n",
        "dx = sigmoid_layer2.backward(dx)\n",
        "dx = hidden_layer.backward(dx)\n",
        "dx = sigmoid_layer.backward(dx)\n",
        "dx = input_layer.backward(dx)\n",
        "print(dx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: [[ 0.  0.  5. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ... 10.  0.  0.]\n",
            " [ 0.  0.  0. ... 16.  9.  0.]\n",
            " ...\n",
            " [ 0.  0.  1. ...  6.  0.  0.]\n",
            " [ 0.  0.  2. ... 12.  0.  0.]\n",
            " [ 0.  0. 10. ... 12.  1.  0.]]\n",
            "Data shape: (1797, 64)\n",
            "X train: [[ 0.  0.  5. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ... 10.  0.  0.]\n",
            " [ 0.  0.  0. ... 16.  9.  0.]\n",
            " ...\n",
            " [ 0.  0.  2. ... 14.  0.  0.]\n",
            " [ 0.  1. 12. ...  0.  0.  0.]\n",
            " [ 0.  0.  0. ...  3.  0.  0.]]\n",
            "X train shape: (898, 64)\n",
            "Epoch 0\n",
            "y: [[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Layer 0 output shape: (1, 2)\n",
            "Layer 1 output shape: (1, 2)\n",
            "Layer 2 output shape: (1, 2)\n",
            "Layer 3 output shape: (1, 2)\n",
            "Layer 4 output shape: (1, 10)\n",
            "Current error: [[-0.96324985]\n",
            " [ 0.22833881]\n",
            " [ 0.44170968]\n",
            " [-0.54363284]\n",
            " [-0.47308932]\n",
            " [ 0.54758993]\n",
            " [-0.50728902]\n",
            " [ 0.85806864]\n",
            " [ 0.9849825 ]\n",
            " [ 0.25544776]]\n",
            "Loss: 0.6870358842002127\n",
            "FullyConnected layer backward\n",
            "output_error_derivative shape: (10, 1)\n",
            "self.weights shape: (2, 10)\n",
            "self.x shape: (1, 2)\n",
            "self.y shape: (1, 10)\n",
            "dx shape: (1, 2)\n",
            "Sigmoid layer backward\n",
            "output_error_derivative shape: (2, 1)\n",
            "self.x shape: (1, 2)\n",
            "self.y shape: (1, 2)\n",
            "local_grad shape: (1, 2)\n",
            "dx shape: (2, 2)\n",
            "FullyConnected layer backward\n",
            "output_error_derivative shape: (2, 2)\n",
            "self.weights shape: (2, 2)\n",
            "self.x shape: (1, 2)\n",
            "self.y shape: (1, 2)\n",
            "dx shape: (2, 2)\n",
            "Sigmoid layer backward\n",
            "output_error_derivative shape: (2, 2)\n",
            "self.x shape: (1, 2)\n",
            "self.y shape: (1, 2)\n",
            "local_grad shape: (1, 2)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[140], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m network \u001b[38;5;241m=\u001b[39m Network(layers, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     38\u001b[0m network\u001b[38;5;241m.\u001b[39mcompile(loss)\n\u001b[0;32m---> 40\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[139], line 141\u001b[0m, in \u001b[0;36mNetwork.fit\u001b[0;34m(self, x_train, y_train, epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m    139\u001b[0m loss_derivative_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mloss_derivative(current_error, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers_reversed:\n\u001b[0;32m--> 141\u001b[0m     loss_derivative_value \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_derivative_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, FullyConnected):\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;66;03m# print(f\"Loss derivative value shape: {loss_derivative_value.shape}\")\u001b[39;00m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# print(f\"weights shape: {layer.weights.shape}\")\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# print(f\"learning_rate: {learning_rate}\")\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         layer\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m loss_derivative_value\n",
            "Cell \u001b[0;32mIn[139], line 73\u001b[0m, in \u001b[0;36mSigmoid.backward\u001b[0;34m(self, output_error_derivative)\u001b[0m\n\u001b[1;32m     71\u001b[0m local_grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_grad shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_grad\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m dx \u001b[38;5;241m=\u001b[39m \u001b[43moutput_error_derivative\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlocal_grad\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdx shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dx\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 2)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "nx = 64\n",
        "n = 2\n",
        "\n",
        "epoch_size = 10\n",
        "digits = datasets.load_digits()\n",
        "n_samples = len(digits.images)\n",
        "data = digits.images.reshape((n_samples, -1))\n",
        "print(f\"Data: {data}\")\n",
        "print(f\"Data shape: {data.shape}\")\n",
        "\n",
        "_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
        "for ax, image, label in zip(axes, digits.images, digits.target):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
        "    ax.set_title(\"Training: %i\" % label)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, digits.target, test_size=0.5, shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"X train: {X_train}\")\n",
        "print(f\"X train shape: {X_train.shape}\")\n",
        "\n",
        "\n",
        "inner_layer = FullyConnected(nx, n)\n",
        "sigmoid_layer_1 = Sigmoid()\n",
        "hidden_layer = FullyConnected(n, n)\n",
        "sigmoid_layer_2 = Sigmoid()\n",
        "out_layer = FullyConnected(n, 10)\n",
        "\n",
        "\n",
        "layers = [inner_layer, sigmoid_layer_1, hidden_layer, sigmoid_layer_2, out_layer]\n",
        "loss_function = lambda e: np.sum(e) ** 2\n",
        "loss_function_derivative = lambda e, weight: 2 * e\n",
        "\n",
        "loss = Loss(loss_function, loss_function_derivative)\n",
        "network = Network(layers, 0.01)\n",
        "network.compile(loss)\n",
        "\n",
        "network.fit(X_train, y_train, 1, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([12, 15, 18])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = np.array([3])\n",
        "b = np.array([[4], [5], [6]])\n",
        "b @ a"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
